{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparison between text proximity and word mover's distance\n",
    "\n",
    "## Purpose\n",
    "blabla\n",
    "\n",
    "## Description of word proximity\n",
    "blabla\n",
    "\n",
    "## Example on Quora duplicate questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from kernel_text_proximity import *\n",
    "\n",
    "df_quora = pd.read_csv(os.path.join(*['data', 'questions.csv'])).dropna().reset_index(drop=True)\n",
    "\n",
    "df_quora.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_RESOURCE_DIRECTORY = os.path.join(os.path.expanduser('~'), 'nlp-resources')\n",
    "word_embedding_file_path = os.path.join(NLP_RESOURCE_DIRECTORY, 'glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['gaussian', 'inverse', 'triangle', 'epanechnikov', 'quadratic', 'cubic', 'circular', 'student']\n",
    "# search_space_l = np.linspace(.01, 1, 1000)\n",
    "# search_space_eps = np.linspace(.001, .1, 1000)\n",
    "search_space_l = np.random.uniform(0, 3, 1000)\n",
    "search_space_eps = np.random.uniform(0, .1, 1000)\n",
    "normalize_values = [True, False]\n",
    "n_sample = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quora_calibration = df_quora.iloc[np.random.choice(list(range(df_quora.shape[0])), size=n_sample, replace=False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for best l and eps values...\n",
      "computing relevant word embeddings...\n",
      "relevant word embeddings computed in 38.5s\n",
      "better parameters found: kernel='triangle', l_best=43.93257756311696, eps_best=0.0998998998998999, normalize=True, corresponding best average precision: 0.2925864120528334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/IdeaProjects/kernel_text_proximity/kernel_text_proximity/kernel_text_proximity.py:109: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  transfo = np.maximum(np.cos(np.pi / (2 * sq_distances)), 0)\n",
      "/home/manuel/IdeaProjects/kernel_text_proximity/kernel_text_proximity/kernel_text_proximity.py:109: RuntimeWarning: invalid value encountered in cos\n",
      "  transfo = np.maximum(np.cos(np.pi / (2 * sq_distances)), 0)\n",
      "/home/manuel/IdeaProjects/kernel_text_proximity/kernel_text_proximity/kernel_text_proximity.py:109: RuntimeWarning: invalid value encountered in maximum\n",
      "  transfo = np.maximum(np.cos(np.pi / (2 * sq_distances)), 0)\n",
      "/home/manuel/IdeaProjects/kernel_text_proximity/kernel_text_proximity/kernel_text_proximity.py:99: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  transfo = np.minimum(1 / sq_distances, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better parameters found: kernel='inverse', l_best=36.490684800331806, eps_best=0.0033033033033033035, normalize=True, corresponding best average precision: 0.3044641013678299\n",
      "better parameters found: kernel='quadratic', l_best=52.85259596150886, eps_best=0.06806806806806807, normalize=True, corresponding best average precision: 0.31695681283823884\n",
      "better parameters found: kernel='quadratic', l_best=47.59190799183409, eps_best=0.07077077077077078, normalize=True, corresponding best average precision: 0.3184070887393715\n",
      "better parameters found: kernel='quadratic', l_best=7.097472026255403, eps_best=0.010510510510510511, normalize=True, corresponding best average precision: 0.3452616098075525\n",
      "better parameters found: kernel='cubic', l_best=2.043705608676616, eps_best=0.04594594594594595, normalize=True, corresponding best average precision: 0.3542405375711724\n",
      "better parameters found: kernel='epanechnikov', l_best=0.398751444818668, eps_best=0.07657657657657659, normalize=True, corresponding best average precision: 0.5645346334479922\n",
      "No better hyperparameters found after 500 trials, break\n",
      "computing relevant word embeddings...\n",
      "relevant word embeddings computed in 36.1s\n",
      "better parameters found: kernel='epanechnikov', l_best=2.043705608676616, eps_best=0.08798798798798799, normalize=False, corresponding best average precision: 0.56717557586312\n",
      "No better hyperparameters found after 500 trials, break\n",
      "best l and eps values out of 8000000 combinations from 1000 pairs computed in 448.8s\n"
     ]
    }
   ],
   "source": [
    "kernel_best, l_best, eps_best, normalize, ap_score_best = calibrate(articles_0=df_quora_calibration['question1'].tolist(),\n",
    "                                                         articles_1=df_quora_calibration['question2'].tolist(),\n",
    "                                                         true_values=df_quora_calibration['is_duplicate'],\n",
    "                                                         word_embedding_file_path=word_embedding_file_path,\n",
    "                                                         l_values=search_space_l,\n",
    "                                                         eps_values=search_space_eps,\n",
    "                                                         normalize_values=normalize_values,\n",
    "                                                         kernels=kernels,\n",
    "                                                         prop=1,\n",
    "                                                         patience=500,\n",
    "                                                         language='english' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing relevant word embeddings...\n",
      "relevant word embeddings computed in 31.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eps': 0.08369580630003995, 'l': 0.0014079800972185618}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = calibrate_bayes(articles_0=df_quora_calibration['question1'].tolist(), \n",
    "                       articles_1=df_quora_calibration['question1'].tolist(), \n",
    "                       true_values=df_quora_calibration['is_duplicate'], \n",
    "                       l_range=(.0001, 1),\n",
    "                       eps_range=(0, .1),\n",
    "                       kernel='gaussian',\n",
    "                       normalize=True,\n",
    "                       word_embedding_file_path=word_embedding_file_path)\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5864306644352056, 0.583831698752922, 0.5831613798984207, 0.586357635549281, 0.5888926818202261, 0.5937553732344659, 0.583647098307573, 0.5885284289679859, 0.5878409117210809, 0.5889681107781775, 0.5898486243441127, 0.5888467567509545, 0.5870966221658249, 0.591849570940923, 0.5935432580315935, 0.5870402904820033, 0.5872175290951873, 0.5866447673988877, 0.5876288069934497, 0.5834419584400814, 0.5825252769803912, 0.5857348549806506, 0.58279487752359, 0.5828241500481972, 0.5874165125256481, 0.5852697969584747, 0.5825900058423341, 0.5874345494583695, 0.5864280601826066, 0.5844607120182862, 0.5831314550356752, 0.586528338426591, 0.5946064133714941, 0.5821741737123021, 0.5839097534936146, 0.5866242171307898, 0.5866656656467866, 0.5841763114754899, 0.5857364690287565, 0.5841338552945293, 0.5865653261821536, 0.5874164332527703, 0.5834773493789698, 0.5871533814001076, 0.5838045079734584, 0.5863572504594181, 0.5942745917405372, 0.5886619822168105, 0.5906902025240807, 0.5859759451678461, 0.5852047624782046, 0.5870722163166736, 0.5833140962178551, 0.5839347377057773, 0.5866554837988329, 0.5870445270825886, 0.5874017948008207, 0.5829486146225908, 0.5870903220710488, 0.587136244978629, 0.5822985010632096, 0.5858740322467657, 0.5845356994568696, 0.5946695504476718, 0.5841715732876269, 0.5829033136598911, 0.582534689407008, 0.5836901521992451, 0.583075005310136, 0.5839223004923956, 0.5857084674670071, 0.5863975170678946, 0.5852734972356683, 0.5866293535625473, 0.5835975705351353, 0.5828946432517742, 0.5856150673224031, 0.5825241366678577, 0.5835261801985296, 0.5841146804572395, 0.5841408231269267, 0.5870929314824918, 0.586724069102203, 0.5877111628781857, 0.5888078650036865, 0.5913028235347434, 0.5865888331234992, 0.5857935076412537, 0.5828346478244995, 0.5885732880733787, 0.5866341956110573, 0.5872900154522857, 0.5864490948256315, 0.5874619066008717, 0.583778987321094, 0.5849856280357826, 0.5840834966370512, 0.5868922009031183, 0.5831641688712348, 0.5863967890488389, 0.5828953999990512, 0.5855387760415878, 0.5872534476590342, 0.5866178835285643, 0.5870761061202316, 0.5844240266571566, 0.582591027178037, 0.5840280169520886, 0.5865009310215529, 0.5947681891575904, 0.5838598435922224, 0.5866152524938116, 0.5830687044543763, 0.586669806641979, 0.5898642474579099, 0.5879624861472041, 0.5857728768774113, 0.5821598051714469, 0.5833683337310169, 0.5840763471210675, 0.5852490372341361, 0.584149650754592, 0.5875630211881763, 0.5829486146225908, 0.5867046519481411, 0.587049710040306, 0.5919796195511358, 0.5865333731683808, 0.5868690129739305, 0.583937834654955, 0.5832123455431619, 0.5840986598127103, 0.5863613292716032, 0.5874301389369067, 0.5829417787584952, 0.585600566904025, 0.5864288951210448, 0.586632783029312, 0.58873289578393, 0.5824846478794852, 0.5890755006606652, 0.585778652237042, 0.5841658268975074, 0.5865510043884241, 0.5870988080365801, 0.5824820091327616, 0.5834524876564723, 0.583075005310136, 0.5846988084753468, 0.5825466052687174, 0.5836782086912801, 0.5840292401685356, 0.5852583723061495, 0.5831713349722575, 0.5828361646627699, 0.5830603986551859, 0.5838226657891238, 0.5834236390438208, 0.5824909945335945, 0.5839681023740906, 0.5856916484959989, 0.5841030019469047, 0.5857320464968829, 0.5830687044543763, 0.5839586395697262, 0.58520958453281, 0.586646514561431, 0.5864240403987605, 0.5831709645967935, 0.5914807259544862, 0.5839868034956972, 0.5940658304318429, 0.5828147211441876, 0.5840055975479932, 0.5834935140922874, 0.5834484548046143, 0.5870498583537247, 0.5861089981285272, 0.5846837856774544, 0.584068802119857, 0.5865886765401355, 0.5855242659918386, 0.5872680319928898, 0.5836178082094899, 0.5888949702232263, 0.5830687044543763, 0.5829405277794546, 0.5874682686989902, 0.5842028806361829, 0.5864110911474195, 0.5874995404605664, 0.5829356694850081, 0.5870723931221988, 0.5838058403499848, 0.5866390541587385, 0.5884927240830209, 0.5852792484058765, 0.5865238879516613, 0.583736664653073, 0.5842354980644481, 0.5857361013503165, 0.5866374845003809, 0.5844583641690708, 0.583860669474145, 0.5872536729272515, 0.5843405836902152, 0.5826094124407056, 0.5853196878773217, 0.5865706286216992, 0.5828572859930001, 0.5843247789791141, 0.583280108864888, 0.585689920414612, 0.5840463475904314, 0.583075005310136, 0.58318679711912, 0.5838409638814438, 0.5847433650300822, 0.5825262339161543, 0.584219658523395, 0.5866206953823798, 0.5866600101888853, 0.5871563163048508, 0.5854830007671156, 0.5864136253029598, 0.5839519821441943, 0.5858769399434218, 0.5831826076986635, 0.5945879951872743, 0.5874286856021903, 0.587416896252971, 0.5869196297596662, 0.5829486146225908, 0.583840663541437, 0.5841113889930117, 0.5839673434964878, 0.5866077245429635, 0.5890201107104535, 0.5821741737123021, 0.5852749051193507, 0.5866816148222064, 0.5933865170247883, 0.5863177141211996, 0.5839223004923956, 0.5828969418708907, 0.583075005310136, 0.5857349548141552, 0.5839454673254172, 0.5872420941112536, 0.5840428254370439, 0.5839885918426351, 0.5839172197047673, 0.5828052999398349, 0.5845870763282552, 0.5864180660984163, 0.5838445562746033, 0.5857592472325063, 0.5828057636579529, 0.5831374306668898, 0.5826140116957683, 0.5831314550356752, 0.5830687044543763, 0.5834213005669029, 0.5829402869192066, 0.5838207012437584, 0.5828746596691956, 0.5851333947580684, 0.5832229631850561, 0.582488123136101, 0.5838461749928103, 0.5830603986551859, 0.5838044022327191, 0.5836375011381567, 0.5830687044543763, 0.5831314550356752, 0.5839884467031489, 0.5837130049465211, 0.5853543707216311, 0.5864254139995608, 0.5832769722348057, 0.5849942937686925, 0.5841104299397515, 0.5830292712727304, 0.5825333821051946, 0.5824795046707381, 0.5864825860312824, 0.5866251765658064, 0.5840864581571958, 0.583075005310136, 0.5838597632693372, 0.5857325684673397, 0.5834608002567279, 0.5865317064960067, 0.5871160818019779, 0.5888194484756855, 0.5852547924242179, 0.5843920775167566, 0.5821734102497496, 0.5836414200457359, 0.5833026826825058, 0.5830687044543763, 0.5875083536956205, 0.5840113825660112, 0.5871050208126144, 0.5847170385069603, 0.5907276866688702, 0.5857567261009102, 0.5941940516657258, 0.5838353291727923, 0.5825269655285402, 0.5841618454573082, 0.5839896487946953, 0.5830687044543763, 0.585273835975949, 0.5865323991803258, 0.5866223290941144, 0.5867270497961578, 0.584103721789959, 0.584035722789192, 0.5841658268975074, 0.583075005310136, 0.5829436222077762, 0.5833985633605868, 0.5946786212313668, 0.5838641157355542, 0.5845924639240802, 0.5870438366524828, 0.5857108474012906, 0.5828976986181678, 0.5839884467031489, 0.5837694226897947, 0.5855334163035704, 0.5841715732876269, 0.5837642562124887, 0.583860669474145, 0.5865519557750135, 0.5864340492925866, 0.5874760809957051, 0.5830687044543763, 0.5887444610279328, 0.583627108304876, 0.5910873430228063, 0.5849959561859202, 0.5866190272830274, 0.5841094755180031, 0.5833179328899687, 0.583075005310136, 0.5841152589758254, 0.5852571716948547, 0.5836351388220906, 0.5841437427376337, 0.5828237481541243, 0.5839370129400454, 0.5857411263503273, 0.5826088713651465, 0.5836437837146289, 0.5928243070430339, 0.5844446427518796, 0.5833704785279039, 0.5881819762837381, 0.5870681923521153, 0.5859622518685501, 0.5839755973556018, 0.5864290608722752, 0.5855209827510333, 0.5828074084317403, 0.5832801088648881, 0.583075005310136, 0.5829215609069709, 0.5849887550043669, 0.583075005310136, 0.5869194038622394, 0.5839981523320997, 0.586998441113362, 0.586638430911219, 0.5833749889428801, 0.5840971261526029, 0.5828273920143189, 0.5874109102851897, 0.5852762160527678, 0.5874277209748994, 0.5832787788765552, 0.5838548395745071, 0.5858627952372955, 0.5868893777972649, 0.5841315184857163, 0.5864255492622314, 0.5824817356316765, 0.5839223004923956, 0.5839669479325718, 0.5828376788416247, 0.583075005310136, 0.5857348956040183, 0.583857002117381, 0.5865569874844813, 0.5844391929485154, 0.586406458811954, 0.5826088713651465, 0.5889231416368311, 0.5852577583667151, 0.5841198812272799, 0.5827989691010877, 0.5824841779969806, 0.5824846478794852, 0.5831314550356752, 0.5836784840138598, 0.5830992563672528, 0.5838232295569363, 0.582591027178037, 0.583075005310136, 0.5828770181419838, 0.583075005310136, 0.5833736162341252, 0.5838703016665882, 0.5837799525817022, 0.5824883766387681, 0.5829930782413655, 0.5844470014804739, 0.5836167298022013, 0.5839223004923957, 0.5825987773093806, 0.5839454673254172, 0.5830603986551859, 0.5829370294011287, 0.5839223004923956, 0.5831877664788063, 0.5849590697168018, 0.5824664042150562, 0.5832036784323531, 0.5837948945822274, 0.5825863619821396, 0.5839847926305745, 0.5830687044543763, 0.5839923517187432, 0.585720034898642, 0.5838248906642148, 0.5821899726785451, 0.5852182249674606, 0.5834647951418825, 0.5835616890466592, 0.5842194736994677, 0.5860321603414825, 0.5829425355057721, 0.5872744912926204, 0.5871206244058546, 0.5830687044543763, 0.5841789859445359, 0.5844753688162143, 0.5837240818515472, 0.5824795046707381, 0.5832247515319939, 0.5852885337102796, 0.584098318038817, 0.5858236591351405, 0.5878891976376938, 0.5840054296651194, 0.5827281869481177, 0.591263346661649, 0.5838287899244554, 0.5830687044543763, 0.5834608002567279, 0.5843182931086669, 0.5849934124236695, 0.5838287061582104, 0.5828361646627699, 0.5887533070542563, 0.5865211258009771, 0.5857029078659993, 0.5828986112430412, 0.5839777085458995, 0.589688905959632, 0.5834608002567279, 0.5825173979746308, 0.5830687044543763, 0.5840072158511688, 0.5852820319906273, 0.5832510810254354, 0.5871070398145575, 0.5867117547753007, 0.5830687044543763, 0.5841676146204591, 0.5865766791460575, 0.5845798109198264, 0.5944295504973502, 0.5836855157511353, 0.5829354307550858, 0.5864032164019861, 0.5885496747869783, 0.5839059066334381, 0.5836300580344623, 0.5839332817847063, 0.5923380616053723, 0.5856892796720862, 0.583075005310136, 0.5828770181419838, 0.5828976986181678, 0.585217619741421, 0.5840869657897161, 0.5866108339578151, 0.583075005310136, 0.5837796772591226, 0.5834935140922874, 0.5941101370432543, 0.582928691514977, 0.5855254559727565, 0.5836775900463305, 0.5825492440154411, 0.5865318048634227, 0.5834608002567279, 0.5839233671838184, 0.584465842150109, 0.5858617226181373, 0.5843395752575042, 0.5821724106325343, 0.5870910367105409, 0.5822925122704687, 0.5841789859445359, 0.583075005310136, 0.5833342201942348, 0.5838721505235517, 0.5852745214437778, 0.5839949502978188, 0.583860669474145, 0.5840533343123542, 0.5864039448928575, 0.5832933327727627, 0.5864793066480711, 0.5828001486162295, 0.5857418510336239, 0.584512600507455, 0.5841105204514807, 0.5839537363932732, 0.5839755973556018, 0.5839893352560953, 0.5827244211820262, 0.5838247227035038, 0.5872997148908463, 0.583075005310136, 0.5840514310370633, 0.5855414333971468, 0.5849831081037083, 0.5839223004923956, 0.5824846478794852, 0.5876953567920843, 0.583075005310136, 0.5839660240910525, 0.5836375195721228, 0.5841789859445359, 0.5864282533229604, 0.5838287899244555, 0.582941997326728, 0.587161209357328, 0.5832798242295252, 0.5852063776737664, 0.5833910530648658, 0.5830687044543763, 0.587416896252971, 0.5839474835532378, 0.5875580233629585, 0.5870158343368921, 0.5858740322467657, 0.5844356962893845, 0.5844259745003841, 0.5866289121640815, 0.5839240834527464, 0.5833029028497085, 0.5840469358932597, 0.5830687044543763, 0.5866715141408105, 0.5845870763282552, 0.5866103596991796, 0.583593516108883, 0.5841112638649409, 0.5857616529125778, 0.5868869248509145, 0.5830687044543763, 0.5841580868192325, 0.5828052999398348, 0.5824846478794852, 0.5834243895073055, 0.5840110447068428, 0.5839181506573943, 0.5824835565801207, 0.583893272652943, 0.5830500741786651, 0.5832972973433629, 0.5827986906487143, 0.5840314680857568, 0.583075005310136, 0.5839570077805888, 0.5843944606369051, 0.5839223004923956, 0.5841403945348116, 0.5839669479325718, 0.5840292051291528, 0.5839272392804644, 0.5821797887540257, 0.5841254100768793, 0.5840763471210675, 0.5830687044543763, 0.5829463050166616, 0.583692459392452, 0.5841763114754899, 0.5838750360647296, 0.5841114706589179, 0.5839749795292373, 0.5829486146225908, 0.5829484419566582, 0.5841268328807664, 0.5840773198283795, 0.5839567008810116, 0.5830687044543763, 0.5832982825906798, 0.5852782012758179, 0.5839454673254172, 0.5849849847750357, 0.583075005310136, 0.5843553456372582, 0.5828026172838792, 0.5841763114754899, 0.5840609049556797, 0.5832574653675535, 0.5840228186627269, 0.5839810233016324, 0.5828273871553805, 0.5889552174950063, 0.5872368742082297, 0.583075005310136, 0.5883254758179541, 0.5838596132920888, 0.5841876353675659, 0.5844331711341904, 0.5838463707814169, 0.5821757035546025, 0.5829500767135045, 0.5842089289037551, 0.5840724499964411, 0.5851815908376455, 0.5821813185963258, 0.5840918591381361, 0.58418811868914, 0.5838409638814438, 0.5840862872012892, 0.5824823581056318, 0.5830687044543763, 0.5857638836848287, 0.5841268328807664, 0.5841947143264961, 0.5841165519394689, 0.5839910651631046, 0.5840744815046078, 0.5824846478794852, 0.5841629238645969, 0.5836564707729203, 0.5866375341480983, 0.5840977758596715, 0.5852203645120058, 0.5839866383433558, 0.5830687044543763, 0.583920254934557, 0.583075005310136, 0.5839550210888954, 0.5864141115271055, 0.5885485657340349, 0.5830687044543763, 0.5840091709661475, 0.5845088367858448, 0.5854612907363823, 0.585800471084976, 0.5841338960251443, 0.5836442130496868, 0.5841629238645969, 0.5832961105501198, 0.5825428450416272, 0.5946801864611291, 0.5840055259880796, 0.5828273920143189, 0.5832866206025669, 0.5841221911612628, 0.5839240834527464, 0.5867699078786731, 0.5830603986551859, 0.5849769461464228, 0.5840748343609703, 0.5830687044543763, 0.5829444616717809, 0.5835951641942474, 0.5838194299971633, 0.5839203755324853, 0.583075005310136, 0.594676813310407, 0.583627108304876, 0.5854551349888595, 0.5864220888886126, 0.5840480134872699, 0.584176233678619, 0.5839223004923957, 0.5843968113318467, 0.5841629238645969, 0.5857145593010201, 0.5828295620043566, 0.5839021295888022, 0.5835267455688373, 0.5910706658454437, 0.5852807622523757, 0.583075005310136, 0.5846370726496135, 0.5821925144929936, 0.5839401455326854, 0.583075005310136, 0.5838990560812873, 0.5857779603374382, 0.5839870565969997, 0.5837582376193384, 0.5872829417023337, 0.5841112638649409, 0.5834458598440492, 0.5843413082589535, 0.5876124643189821, 0.583075005310136, 0.5838638658653601, 0.5838194299971633, 0.583075005310136, 0.5887704855892886, 0.5865254580534467, 0.5844518948679948, 0.5841773913357335, 0.583627108304876, 0.5889802122471569, 0.5852469973884019, 0.5828964412530033, 0.5839827026620195, 0.5902947129642656, 0.5866221011532207, 0.5841629238645969, 0.5834484548046143, 0.583075005310136, 0.5876892004078783, 0.5836454807926188, 0.5831510322877307, 0.5840918591381361, 0.5865356034302933, 0.5843875202562994, 0.5832957467836168, 0.5856944153510433, 0.5825267754145813, 0.5842031031750687, 0.5836413994429256, 0.5849081148347635, 0.5867165902668651, 0.583075005310136, 0.5874044918923959, 0.5855564788426586, 0.5821782830504476, 0.5831339044340497, 0.5866431531501074, 0.5839233671838184, 0.5870758628436354, 0.583814362185996, 0.5828056092381375, 0.583075005310136, 0.5862030145663271, 0.5833231704374031, 0.5853008397960147, 0.5840760026294627, 0.5841425598368866, 0.583807498438017, 0.583075005310136, 0.5839213942875876, 0.5828508439530299, 0.5838183501369196, 0.5825255769400923, 0.584080111299248, 0.5839196059406497, 0.5840645079521645, 0.5840446848893243, 0.5844183736121185, 0.5857594521382244, 0.5841779075372472, 0.5824846478794852, 0.5830687044543763, 0.5871709458895757, 0.5849590590935976, 0.5874829894916715, 0.583075005310136, 0.583379774082447, 0.5824887635726561, 0.5840157120269046, 0.5839525540486318, 0.582183594174269, 0.5839551854340999, 0.5824909739850972, 0.5839537363932732, 0.5834458598440492, 0.5839240834527464, 0.5830603986551859, 0.5841403945348116, 0.5825267754145813, 0.5836448297631941, 0.5840604396187721, 0.583075005310136, 0.5833913994337976, 0.5841629238645969, 0.583075005310136, 0.5829402869192064, 0.5841104299397515, 0.5838290758630353, 0.583075005310136, 0.5840960485585927, 0.5828018494987299, 0.5839531295737229, 0.583075005310136, 0.5829266032322988, 0.5839240834527464, 0.5838449420616489, 0.5833931877807355, 0.5830687044543763, 0.583075005310136, 0.5838721505235517, 0.5840698275968064, 0.5839199366203665, 0.5829402869192064, 0.583364864747703, 0.5822965298816807, 0.5841112638649409, 0.5838763159985951, 0.583921118965008, 0.5836442130496868, 0.5839802865070792, 0.5840103041587226, 0.5839283028384608, 0.5840881251150093, 0.5824817150831791, 0.5841923735554289, 0.5828572859930001, 0.5839367547947114, 0.5830687044543763, 0.5841811969300829, 0.583627108304876, 0.5841580132556772, 0.5821749952554678, 0.5938277874376385, 0.583075005310136, 0.5849797364169592, 0.583075005310136, 0.5840275784710516, 0.5838915661694438, 0.5836894357142415, 0.5828953999990512, 0.5832770052352709, 0.5840805807510898, 0.5840082874712949, 0.5825466270260098, 0.5839525540486317, 0.5844306834724502, 0.5852573878107058, 0.5844201619590563, 0.583075005310136, 0.5836430965209155, 0.5839160355488425, 0.5839910651631046, 0.5833000101411794, 0.5828976986181676, 0.5825316984291384, 0.5833134893524748, 0.5839391219132662, 0.5830687044543763, 0.5863470501912678, 0.5839199366203666, 0.5834458598440492, 0.5844155487486505, 0.582168481513456, 0.5840977758596714, 0.5836941654127061, 0.5833007469357328, 0.583075005310136, 0.5839223004923957, 0.5852311294583062, 0.5828953999990512, 0.5830687044543763, 0.5833118088635558, 0.5840805807510898, 0.5840675279702617, 0.5839241560060808, 0.5830644634479831, 0.5839949502978188, 0.5830687044543763, 0.5841629238645969, 0.5855266710534569, 0.5839474835532378, 0.5844206021292592, 0.583075005310136, 0.583284793985249, 0.5868096131898894, 0.5829444616717809, 0.5836431171237259, 0.5874301389369067, 0.5841876353675659, 0.5836278450994292, 0.583075005310136, 0.5851692818732366, 0.5840533343123542, 0.5925475416437761, 0.5833917898594191, 0.5839199366203666, 0.5840763471210675, 0.5836884216118317, 0.5857628079129578, 0.5864373691260505, 0.5828440506306252, 0.5834608002567279, 0.5840677419076032, 0.5841432966314397, 0.5828273920143189, 0.5848476267912293, 0.5872655402824585, 0.5841724447865511, 0.5839278071364211, 0.5840373060531672, 0.5840433448782734, 0.5836260549132845, 0.5830687044543763, 0.5840228186627269, 0.5859613444050951, 0.5821843576368215, 0.5854429762718365, 0.5841870410951298, 0.5843413082589534, 0.5839474835532378, 0.5841934534156724, 0.583075005310136, 0.5838063844017152, 0.5841105733708756, 0.5843560702059963, 0.5829437116074736, 0.583075005310136, 0.5885667735534055, 0.5947737396373899, 0.5851787856701519, 0.5841618454573082, 0.5898449150076746, 0.591709310341721, 0.583075005310136, 0.5834935140922874, 0.5834458318647171, 0.5840853189389528, 0.5832300055014206, 0.5836437837146289, 0.5871037365867138, 0.582823401037046, 0.5840382912329756, 0.5837784949144811, 0.5825262339161543, 0.5866497797192105, 0.5837268976067033, 0.5877838325770683, 0.5857532741880214, 0.5835788921312024, 0.5834327861768037, 0.5839474835532378, 0.5847967301322867, 0.583075005310136, 0.5839230924480847, 0.5830687044543763, 0.585284203728325, 0.5873709739018396, 0.5838544045305922, 0.5844242523248768, 0.583830094216056, 0.583075005310136, 0.5833124416577903, 0.5824840578124217, 0.5839550210888954, 0.5834450950701637, 0.5832760348788453, 0.5869859418882507, 0.582485478015593, 0.5836278450994292, 0.5847149883291719, 0.5864243082470938, 0.58318679711912, 0.5836508360642688, 0.5831349702077917, 0.5824653753607952, 0.5841629238645969, 0.586614744524086, 0.5828953999990512, 0.5838918237032582, 0.5852112567434382, 0.583465848533474, 0.583075005310136, 0.5857640157650104, 0.5840480134872699, 0.5875307955939372, 0.5824846478794852, 0.5839933249503191, 0.5841392836383371, 0.5828508439530299, 0.584447954012618, 0.583622778039018]\n"
     ]
    }
   ],
   "source": [
    "print(trials.losses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'losses'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6d8ba8671870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'losses'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "best.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "model loaded in 407.7s\n",
      "average precision score with wmd: 0.6014157734331962\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import  get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "stop_words = get_stop_words(language='english')\n",
    "\n",
    "sentences_0 = [[w for w in re.findall(r'\\w+', sentence.lower()) if w not in stop_words] for sentence in df_quora_calibration['question1'].tolist()]\n",
    "sentences_1 = [[w for w in re.findall(r'\\w+', sentence.lower()) if w not in stop_words] for sentence in df_quora_calibration['question2'].tolist()]\n",
    "\n",
    "NLP_RESOURCE_DIRECTORY = os.path.join(os.path.expanduser('~'), 'nlp-resources')\n",
    "tmp_file = get_tmpfile(os.path.join(NLP_RESOURCE_DIRECTORY, 'word2vec.840B.300d.txt'))\n",
    "\n",
    "print('loading model...')\n",
    "t0 = time.time()\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "print('model loaded in {}s'.format(round(time.time() - t0, 1)))\n",
    "\n",
    "wmd_distances = []\n",
    "for i in range(n_sample):\n",
    "    # model.wmdistance requires package pyemd\n",
    "    wmd_distance = model.wmdistance(sentences_0[i], sentences_1[i])\n",
    "    wmd_distances.append(wmd_distance)\n",
    "    \n",
    "min_wmd_distance, max_wmd_distance = min(wmd_distances), max([wd for wd in wmd_distances if wd < np.inf])\n",
    "scores = pd.Series([(d - min_wmd_distance) / (max_wmd_distance - min_wmd_distance) for d in wmd_distances])\n",
    "scores = 1 - scores\n",
    "\n",
    "aps = average_precision_score(df_quora_calibration['is_duplicate'][scores > -np.inf], scores[scores > -np.inf])\n",
    "print('average precision score with wmd: {}'.format(aps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter used:\n",
      " * l: 2.043705608676616\n",
      " * eps: 0.08798798798798799\n",
      " * kernel: epanechnikov\n",
      " * normalize: False\n",
      "computing relevant word embeddings...\n",
      "relevant word embeddings computed in 644.4s\n",
      "computing pair scores...\n",
      "scores for 404348 pairs computed in 85.8s\n",
      "\n",
      "average precision score with kernel text proximity: 0.53\n"
     ]
    }
   ],
   "source": [
    "print('hyperparameter used:\\n * l: {}\\n * eps: {}\\n * kernel: {}\\n * normalize: {}'.format(l_best, eps_best, kernel_best, normalize))\n",
    "\n",
    "scores = evaluate(articles_0=df_quora['question1'].tolist(),\n",
    "         articles_1=df_quora['question2'].tolist(),\n",
    "         l=l_best, \n",
    "         eps=eps_best, \n",
    "         word_embedding_file_path=word_embedding_file_path, \n",
    "         kernel=kernel_best, \n",
    "         normalize=normalize,\n",
    "         language='english')\n",
    "\n",
    "ap_score = average_precision_score(df_quora['is_duplicate'][scores >= 0], scores[scores >= 0])\n",
    "\n",
    "print('\\naverage precision score with kernel text proximity: {}'.format(round(ap_score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1e2ec645fa23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m scores = evaluate(articles_0=df_quora['question1'].tolist(),\n\u001b[1;32m      2\u001b[0m          \u001b[0marticles_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_quora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0mword_embedding_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_embedding_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "scores = evaluate(articles_0=df_quora['question1'].tolist(),\n",
    "         articles_1=df_quora['question2'].tolist(),\n",
    "         l=best['l'], \n",
    "         eps=best['eps'], \n",
    "         word_embedding_file_path=word_embedding_file_path, \n",
    "         kernel='gaussian', \n",
    "         normalize=True,\n",
    "         language='english')\n",
    "\n",
    "ap_score = average_precision_score(df_quora['is_duplicate'][scores >= 0], scores[scores >= 0])\n",
    "\n",
    "print('\\naverage precision score with kernel text proximity: {}'.format(round(ap_score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing wmd distances...\n",
      "wmd distances for 404348 paires computed in 259.2s\n",
      "average precision score with wmd: 0.575\n"
     ]
    }
   ],
   "source": [
    "sentences_0 = [[w for w in re.findall(r'\\w+', sentence.lower()) if w not in stop_words] for sentence in df_quora['question1'].tolist()]\n",
    "sentences_1 = [[w for w in re.findall(r'\\w+', sentence.lower()) if w not in stop_words] for sentence in df_quora['question2'].tolist()]\n",
    "\n",
    "print('computing wmd distances...')\n",
    "t0 = time.time()\n",
    "wmd_distances = []\n",
    "for i in range(df_quora.shape[0]):\n",
    "    wmd_distance = model.wmdistance(sentences_0[i], sentences_1[i])\n",
    "    wmd_distances.append(wmd_distance)\n",
    "print('wmd distances for {} paires computed in {}s'.format(df_quora.shape[0], round(time.time() - t0, 1)))\n",
    "    \n",
    "min_wmd_distance, max_wmd_distance = min(wmd_distances), max([wd for wd in wmd_distances if wd < np.inf])\n",
    "scores = pd.Series([(d - min_wmd_distance) / (max_wmd_distance - min_wmd_distance) for d in wmd_distances])\n",
    "scores = 1 - scores\n",
    "\n",
    "aps = average_precision_score(df_quora['is_duplicate'][scores > -np.inf], scores[scores > -np.inf])\n",
    "print('average precision score with wmd: {}'.format(round(aps, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_ = np.array([1, 2, 3])\n",
    "arr = np.ones(shape=(4, 3))\n",
    "np.multiply(arr, arr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
